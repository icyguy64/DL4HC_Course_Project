{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation with BERT\n",
    "\n",
    "After ETL and text-processing, we continue to model training and evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script will only train one of the 5 BERT models. The INDEX_SPECIFIED parameter will indicate the index of the model as well as the text inputs to extract from the input texts. For each iteration, you'll have to run preprocessing once and this notebook. I'll have a separate evaluate notebook to combine the results together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8153</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51131</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55065</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7368</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29489</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TEXT\n",
       "8153      \n",
       "51131     \n",
       "55065     \n",
       "7368      \n",
       "29489     "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INDEX_SPECIFIED = 4 \n",
    "\n",
    "INDEX_L = int(45616*INDEX_SPECIFIED)\n",
    "INDEX_R = int(45616*(INDEX_SPECIFIED+1))\n",
    "\n",
    "INDEX_L_VAL = int(11404*INDEX_SPECIFIED)\n",
    "INDEX_R_VAL = int(11404*(INDEX_SPECIFIED+1))\n",
    "\n",
    "SAVE_MODEL_NAME = 'model_fifth500only1epoch.bert'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will utilize NLTK's stopwords library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/yaolong/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The necessary imports for our BERT model to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "words = stopwords.words(\"english\")\n",
    "lemma = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from transformers import RobertaTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel,RobertaModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "MAX_LEN = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard preprocessing steps needed for BERT. We need to perform text-preprocessing which was already done earlier but I do it again and also preprocessing specific to BERT to tokenize the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Bert tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\",do_lower_case=True)\n",
    "# Create a funcition to tokenize a set of text\n",
    "\n",
    "def preprocessing_for_bert(data):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    #for every sentence...\n",
    "    \n",
    "    for sent in data:\n",
    "        # 'encode_plus will':\n",
    "        # (1) Tokenize the sentence\n",
    "        # (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        # (3) Truncate/Pad sentence to max length\n",
    "        # (4) Map tokens to their IDs\n",
    "        # (5) Create attention mask\n",
    "        # (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text = text_preprocessing(sent),   #preprocess sentence\n",
    "            add_special_tokens = True,         #Add `[CLS]` and `[SEP]`\n",
    "            max_length= MAX_LEN  ,             #Max length to truncate/pad\n",
    "            pad_to_max_length = True,          #pad sentence to max length \n",
    "            return_attention_mask= True        #Return attention mask \n",
    "        )\n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "        \n",
    "    #convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "    \n",
    "    return input_ids,attention_masks\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    - Remove entity mentions (eg. '@united')\n",
    "    - Correct errors (eg. '&amp;' to '&')\n",
    "    @param    text (str): a string to be processed.\n",
    "    @return   text (Str): the processed string.\n",
    "    \"\"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"won't\", \"will not \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub(r\"\\'\\n\", \" \", text)\n",
    "    text = re.sub(r\"-\", \" \", text)\n",
    "    text = re.sub(r\"\\'\\xa0\", \" \", text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = ''.join(c for c in text if not c.isnumeric())\n",
    "    \n",
    "    # Remove '@name'\n",
    "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "\n",
    "    # Replace '&amp;' with '&'\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "\n",
    "    # Remove trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the training and validation inputs and masks required for our BERT input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yaolong/anaconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Run function 'preprocessing_for_bert' on the train set and validation set\n",
    "print('Tokenizing data...')\n",
    "train_inputs, train_masks = preprocessing_for_bert(X_train['TEXT'])\n",
    "val_inputs, val_masks = preprocessing_for_bert(X_val['TEXT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to convert the labels to torch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert other data types to torch.Tensor\n",
    "train_labels = torch.tensor(y_train.values)\n",
    "val_labels = torch.tensor(y_val.values)\n",
    "\n",
    "## For fine-tuning Bert, the authors recommmend a batch size of 16 or 32\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are actually 45,616 entries in our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45616"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_printoptions(profile='full')\n",
    "\n",
    "len(train_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe the size of the training input, masks and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([45616, 500])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([45616, 500])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_masks.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([45616, 50])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch.set_printoptions(profile='full')\n",
    "train_labels.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We jave to create the DataLoader for our training and validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs,train_masks, train_labels)\n",
    "#train_sampler = RandomSampler(train_data)\n",
    "#train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "#val_sampler = SequentialSampler(val_data)\n",
    "#val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe the length of the train_dataloader, train_data, val_dat and val_dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11404"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45616"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11404"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2851"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the boiler code of BERT model for classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 311 µs, sys: 9 µs, total: 320 µs\n",
      "Wall time: 35 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create the BertClassifier class\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "        Bert Model for classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        \"\"\"\n",
    "        @param   bert: a BertModel object\n",
    "        @param   classifier: a torch.nn.Module classifier\n",
    "        @param   freeze_bert (bool): Set `False` to fine_tune the Bert model\n",
    "        \"\"\"\n",
    "        super(BertClassifier,self).__init__()\n",
    "        # Specify hidden size of Bert, hidden size of our classifier, and number of labels\n",
    "        D_in, H,D_out = 768, 30, 50\n",
    "        \n",
    "#         self.bert = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "                            nn.Linear(D_in, H),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(H, D_out))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # Freeze the Bert Model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    def forward(self,input_ids,attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                           attention_mask = attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:,0,:]\n",
    "        \n",
    "        # Feed input to classifier to compute logits\n",
    "        logit = self.classifier(last_hidden_state_cls)\n",
    "        \n",
    "#         logits = self.sigmoid(logit)\n",
    "        \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following function, we initialize our BERT model with the optimizer and learning rate scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(epochs=4):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = BertClassifier(freeze_bert=False)\n",
    "    \n",
    "    bert_classifier.to(device)\n",
    "    \n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                     lr=5e-5, #Default learning rate\n",
    "                     eps=1e-8 #Default epsilon value\n",
    "                     )\n",
    "    \n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    \n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                              num_warmup_steps=0, # Default value\n",
    "                                              num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make use of the same utility function as LR/CNN/BiGRU/CAML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Helper class that I found online, it's pretty good. Just computes the average. \n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "#  referenced from James Mullenbach, https://github.com/jamesmullenbach/caml-mimic/\n",
    "def union_size(yhat, y, axis):\n",
    "    # axis=0 for label-level union (macro). axis=1 for instance-level\n",
    "    return np.logical_or(yhat, y).sum(axis=axis).astype(float)\n",
    "\n",
    "#  referenced from James Mullenbach, https://github.com/jamesmullenbach/caml-mimic/\n",
    "def intersect_size(yhat, y, axis):\n",
    "    # axis=0 for label-level union (macro). axis=1 for instance-level\n",
    "    return np.logical_and(yhat, y).sum(axis=axis).astype(float)\n",
    "\n",
    "#  referenced from James Mullenbach, https://github.com/jamesmullenbach/caml-mimic/evaluation.py\n",
    "def macro_accuracy(yhat, y):\n",
    "    num = intersect_size(yhat, y, 0) / (union_size(yhat, y, 0) + 1e-10)\n",
    "    return np.mean(num)\n",
    "\n",
    "#  referenced from James Mullenbach, https://github.com/jamesmullenbach/caml-mimic/evaluation.py\n",
    "def macro_precision(yhat, y):\n",
    "    num = intersect_size(yhat, y, 0) / (yhat.sum(axis=0) + 1e-10)\n",
    "    return np.mean(num)\n",
    "\n",
    "#  referenced from James Mullenbach, https://github.com/jamesmullenbach/caml-mimic/evaluation.py\n",
    "def macro_recall(yhat, y):\n",
    "    num = intersect_size(yhat, y, 0) / (y.sum(axis=0) + 1e-10)\n",
    "    return np.mean(num)\n",
    "\n",
    "#  referenced from James Mullenbach, https://github.com/jamesmullenbach/caml-mimic/evaluation.py\n",
    "def macro_f1(yhat, y):\n",
    "    prec = macro_precision(yhat, y)\n",
    "    rec = macro_recall(yhat, y)\n",
    "    if prec + rec == 0:\n",
    "        f1 = 0.\n",
    "    else:\n",
    "        f1 = 2 * (prec * rec) / (prec + rec)\n",
    "    return f1\n",
    "\n",
    "#  referenced from James Mullenbach, https://github.com/jamesmullenbach/caml-mimic/evaluation.py\n",
    "def micro_accuracy(yhatmic, ymic):\n",
    "    return intersect_size(yhatmic, ymic, 0) / union_size(yhatmic, ymic, 0)\n",
    "\n",
    "#  referenced from James Mullenbach, https://github.com/jamesmullenbach/caml-mimic/evaluation.py\n",
    "def micro_precision(yhatmic, ymic):\n",
    "    return intersect_size(yhatmic, ymic, 0) / yhatmic.sum(axis=0)\n",
    "\n",
    "#  referenced from James Mullenbach, https://github.com/jamesmullenbach/caml-mimic/evaluation.py\n",
    "def micro_recall(yhatmic, ymic):\n",
    "    return intersect_size(yhatmic, ymic, 0) / ymic.sum(axis=0)\n",
    "\n",
    "#  referenced from James Mullenbach, https://github.com/jamesmullenbach/caml-mimic/evaluation.py\n",
    "def micro_f1(yhatmic, ymic):\n",
    "    prec = micro_precision(yhatmic, ymic)\n",
    "    rec = micro_recall(yhatmic, ymic)\n",
    "    if prec + rec == 0:\n",
    "        f1 = 0.\n",
    "    else:\n",
    "        f1 = 2 * (prec * rec) / (prec + rec)\n",
    "    return f1\n",
    "\n",
    "#  referenced from James Mullenbach, https://github.com/jamesmullenbach/caml-mimic/evaluation.py\n",
    "def inst_precision(yhat, y):\n",
    "    num = intersect_size(yhat, y, 1) / yhat.sum(axis=1)\n",
    "    # correct for divide-by-zeros\n",
    "    num[np.isnan(num)] = 0.\n",
    "    return np.mean(num)\n",
    "\n",
    "#  referenced from James Mullenbach, https://github.com/jamesmullenbach/caml-mimic/evaluation.py\n",
    "def inst_recall(yhat, y):\n",
    "    num = intersect_size(yhat, y, 1) / y.sum(axis=1)\n",
    "    # correct for divide-by-zeros\n",
    "    num[np.isnan(num)] = 0.\n",
    "    return np.mean(num)\n",
    "\n",
    "#  referenced from James Mullenbach, https://github.com/jamesmullenbach/caml-mimic/evaluation.py\n",
    "def inst_f1(yhat, y):\n",
    "    prec = inst_precision(yhat, y)\n",
    "    rec = inst_recall(yhat, y)\n",
    "    f1 = 2 * (prec * rec) / (prec + rec)\n",
    "    return f1\n",
    "\n",
    "#  referenced from James Mullenbach, https://github.com/jamesmullenbach/caml-mimic/evaluation.py\n",
    "def precision_at_k(yhat_raw, y, k):\n",
    "    # num true labels in top k predictions / k\n",
    "    sortd = np.argsort(yhat_raw)[:, ::-1]\n",
    "    topk = sortd[:, :k]\n",
    "\n",
    "    # get precision at k for each example\n",
    "    vals = []\n",
    "    for i, tk in enumerate(topk):\n",
    "        if len(tk) > 0:\n",
    "            num_true_in_top_k = y[i, tk].sum()\n",
    "            denom = len(tk)\n",
    "            vals.append(num_true_in_top_k / float(denom))\n",
    "\n",
    "    return np.mean(vals)\n",
    "\n",
    "\n",
    "#  referenced from James Mullenbach, https://github.com/jamesmullenbach/caml-mimic/evaluation.py\n",
    "def auc_metrics(yhat_raw, y, ymic):\n",
    "    if yhat_raw.shape[0] <= 1:\n",
    "        return\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = {}\n",
    "    # get AUC for each label individually\n",
    "    relevant_labels = []\n",
    "    auc_labels = {}\n",
    "    for i in range(y.shape[1]):\n",
    "        # only if there are true positives for this label\n",
    "        if y[:, i].sum() > 0:\n",
    "            fpr[i], tpr[i], _ = roc_curve(y[:, i], yhat_raw[:, i])\n",
    "            if len(fpr[i]) > 1 and len(tpr[i]) > 1:\n",
    "                auc_score = auc(fpr[i], tpr[i])\n",
    "                if not np.isnan(auc_score):\n",
    "                    auc_labels[\"auc_%d\" % i] = auc_score\n",
    "                    relevant_labels.append(i)\n",
    "\n",
    "    # macro-AUC: just average the auc scores\n",
    "    aucs = []\n",
    "    for i in relevant_labels:\n",
    "        aucs.append(auc_labels['auc_%d' % i])\n",
    "    roc_auc['auc_macro'] = np.mean(aucs)\n",
    "\n",
    "    # micro-AUC: just look at each individual prediction\n",
    "    yhatmic = yhat_raw.ravel()\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(ymic, yhatmic)\n",
    "    roc_auc[\"auc_micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify the loss function as well as the seed. The train and evaluate functions of the BERT model. This is modified from a standard template of BERT for classification tasks. The evalute function will also save the predictions as well as the actual labels in separate files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify loss function\n",
    "#loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False, train=True, save='model.best'):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    if train==True:\n",
    "        print(\"Start training...\\n\")\n",
    "        for epoch_i in range(epochs):\n",
    "            # =======================================\n",
    "            #               Training\n",
    "            # =======================================\n",
    "            # Print the header of the result table\n",
    "            print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "            print(\"-\"*70)\n",
    "\n",
    "            # Measure the elapsed time of each epoch\n",
    "            t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "            # Reset tracking variables at the beginning of each epoch\n",
    "            total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "            # Put the model into the training mode\n",
    "            model.train()\n",
    "\n",
    "            # For each batch of training data...\n",
    "            for step, batch in enumerate(train_dataloader):\n",
    "                batch_counts +=1\n",
    "                # Load batch to GPU\n",
    "                b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "                  \n",
    "                # Zero out any previously calculated gradients\n",
    "                model.zero_grad()\n",
    "\n",
    "                # Perform a forward pass. This will return logits.\n",
    "                logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "                # Compute loss and accumulate the loss values\n",
    "                loss = loss_fn(logits, b_labels.float())\n",
    "                batch_loss += loss.item()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Perform a backward pass to calculate gradients\n",
    "                loss.backward()\n",
    "\n",
    "                # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "                # Update parameters and the learning rate\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                # Print the loss values and time elapsed for every 20--50000 batches\n",
    "                if (step % 50000 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                    # Calculate time elapsed for 20 batches\n",
    "                    time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                    # Print training results\n",
    "                    print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                    # Reset batch tracking variables\n",
    "                    batch_loss, batch_counts = 0, 0\n",
    "                    t0_batch = time.time()\n",
    "\n",
    "            # Calculate the average loss over the entire training data\n",
    "            avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "            print(\"-\"*70)\n",
    "            \n",
    "        torch.save(model, save)\n",
    "        print(\"Training complete!\")\n",
    "    else:\n",
    "        model = torch.load(save)\n",
    "\n",
    "    # =======================================\n",
    "    #               Evaluation\n",
    "    # =======================================\n",
    "    if evaluation == True:\n",
    "        # After the completion of each training epoch, measure the model's performance\n",
    "        # on our validation set.\n",
    "        val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "        # Print performance over the entire training data\n",
    "        #time_elapsed = time.time() - t0_epoch\n",
    "        print(f\"{val_loss:^10.6f} | {val_accuracy:^9.2f}\")    \n",
    "        #print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "        #print(\"-\"*70)\n",
    "    #print(\"\\n\")\n",
    "\n",
    "    # Always return model after training\n",
    "    #return model\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader, to_save=True):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # Prediction variables\n",
    "    y_hats = []\n",
    "    y_all = []\n",
    "    yhat_raws = []\n",
    "    \n",
    "    # For each batch in our validation set...\n",
    "    for idx, batch in enumerate(val_dataloader):\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        #print(b_attn_mask.size())\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        #print(b_labels)\n",
    "        #print(logits)\n",
    "        #print(b_input_ids[:,0:500])\n",
    "        #print(b_attn_mask[:,0:500])\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels.float())\n",
    "        val_loss.append(loss.item())\n",
    "        #print(loss)\n",
    "        # Get the predictions\n",
    "        #preds = torch.argmax(logits, dim=1).flatten()\n",
    "        #print(preds)\n",
    "        #break\n",
    "        # Calculate the accuracy rate\n",
    "        #accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        accuracy = accuracy_thresh(logits.view(-1,50),b_labels.view(-1,50))\n",
    "        \n",
    "        val_accuracy.append(accuracy)\n",
    "        \n",
    "        # Calculate y, yhat, yhat_raw\n",
    "        y = b_labels.cpu().detach().numpy()\n",
    "        y_all.append(y)\n",
    "        yhat = torch.sigmoid(logits).cpu().detach().round().numpy()\n",
    "        y_hats.append(yhat)\n",
    "        yhat_raw = torch.sigmoid(logits).cpu().detach().numpy()\n",
    "        yhat_raws.append(yhat_raw)        \n",
    "\n",
    "    y_hats = np.concatenate(y_hats, axis=0)\n",
    "    y_all = np.concatenate(y_all, axis=0)\n",
    "    yhat_raws = np.concatenate(yhat_raws, axis=0)\n",
    "    \n",
    "    from numpy import savetxt\n",
    "    if to_save == True:\n",
    "        if INDEX_SPECIFIED == 0:\n",
    "            savetxt('y_hat_0.csv', y_hats, delimiter=',')\n",
    "            savetxt('y_all_0.csv', y_all, delimiter=',')\n",
    "            savetxt('yhat_raws_0.csv', yhat_raws, delimiter=',')\n",
    "        elif INDEX_SPECIFIED == 1:\n",
    "            savetxt('y_hat_500.csv', y_hats, delimiter=',')\n",
    "            savetxt('y_all_500.csv', y_all, delimiter=',')\n",
    "            savetxt('yhat_raws_500.csv', yhat_raws, delimiter=',')\n",
    "        elif INDEX_SPECIFIED == 2:\n",
    "            savetxt('y_hat_1000.csv', y_hats, delimiter=',')\n",
    "            savetxt('y_all_1000.csv', y_all, delimiter=',')\n",
    "            savetxt('yhat_raws_1000.csv', yhat_raws, delimiter=',')\n",
    "        elif INDEX_SPECIFIED == 3:\n",
    "            savetxt('y_hat_1500.csv', y_hats, delimiter=',')\n",
    "            savetxt('y_all_1500.csv', y_all, delimiter=',')\n",
    "            savetxt('yhat_raws_1500.csv', yhat_raws, delimiter=',')\n",
    "        elif INDEX_SPECIFIED == 4:\n",
    "            savetxt('y_hat_2000.csv', y_hats, delimiter=',')\n",
    "            savetxt('y_all_2000.csv', y_all, delimiter=',')\n",
    "            savetxt('yhat_raws_2000.csv', yhat_raws, delimiter=',')\n",
    "\n",
    "        #print('y_hats')\n",
    "    #print(y_hats.shape)\n",
    "    num_rows = int(y_hats.shape[0])\n",
    "    #num_cols = int(y_hats.shape[1])\n",
    "    #print(num_rows)\n",
    "    #print(num_cols)\n",
    "    num_rows_batch_1 = int(y_hats.shape[0]/5)\n",
    "    num_rows_batch_2 = int(y_hats.shape[0]/5*2)\n",
    "    num_rows_batch_3 = int(y_hats.shape[0]/5*3)\n",
    "    num_rows_batch_4 = int(y_hats.shape[0]/5*4)\n",
    "    num_rows_batch_5 = int(y_hats.shape[0]/5*5)\n",
    "    \n",
    "    y_hats = y_hats[0:num_rows_batch_1,:] #+ y_hats[num_rows_batch_1:num_rows_batch_2,:] + y_hats[num_rows_batch_2:num_rows_batch_3,:] + y_hats[num_rows_batch_3:num_rows_batch_4,:] + y_hats[num_rows_batch_4:num_rows,:]\n",
    "    y_all = y_all[0:num_rows_batch_1,:] #+ y_all[num_rows_batch_1:num_rows_batch_2,:] + y_all[num_rows_batch_2:num_rows_batch_3,:] + y_all[num_rows_batch_3:num_rows_batch_4,:] + y_all[num_rows_batch_4:num_rows,:]\n",
    "    yhat_raws = yhat_raws[0:num_rows_batch_1,:] #+ yhat_raws[num_rows_batch_1:num_rows_batch_2,:] + yhat_raws[num_rows_batch_2:num_rows_batch_3,:] + yhat_raws[num_rows_batch_3:num_rows_batch_4,:] + yhat_raws[num_rows_batch_4:num_rows,:]\n",
    "    \n",
    "    #y_hats[y_hats > 1] = 1\n",
    "    #y_all[y_all > 1] = 1\n",
    "    #yhat_raws[yhat_raws > 1] = 1\n",
    "    \n",
    "    mac_acc = macro_accuracy(y_hats, y_all)\n",
    "    mac_rec = macro_recall(y_hats, y_all)\n",
    "    mac_pre = macro_precision(y_hats, y_all)\n",
    "    mic_acc = micro_accuracy(y_hats.ravel(), y_all.ravel())\n",
    "    mic_rec = micro_recall(y_hats.ravel(), y_all.ravel())\n",
    "    mic_pre = micro_precision(y_hats.ravel(), y_all.ravel())\n",
    "    mic_f1 = micro_f1(y_hats.ravel(), y_all.ravel())\n",
    "    mac_f1 = macro_f1(y_hats, y_all)\n",
    "    auc_dict = auc_metrics(yhat_raws, y_all, y_all.ravel())\n",
    "    prec_at_5 = precision_at_k(yhat_raws, y_all, 5)\n",
    "    prec_at_8 = precision_at_k(yhat_raws, y_all, 8)\n",
    "    prec_at_15 = precision_at_k(yhat_raws, y_all, 15)\n",
    "\n",
    "    print('Loss {loss:.4f} \\t'\n",
    "          'Macro Accuracy {mac_acc:.3f} \\t'\n",
    "          'Macro Recall {mac_rec:.3f} \\t'\n",
    "          'Macro Precision {mac_pre:.3f} \\t'\n",
    "          'Macro F1 {mac_f1:.3f} \\t'\n",
    "          'Macro AUC {mac_auc:.3f} \\t'\n",
    "          'Micro Accuracy {mic_acc:.3f} \\t'\n",
    "          'Micro Recall {mic_rec:.3f} \\t'\n",
    "          'Micro Precision {mic_pre:.3f} \\t'\n",
    "          'Micro F1 {mic_f1:.3f} \\t'\n",
    "          'Micro AUC {auc_micro:.3f} \\t'\n",
    "          'P@5 {prec_at_5:.3f} \\t'\n",
    "          'P@8 {prec_at_8:.3f} \\t'\n",
    "          'P@15 {prec_at_15:.3f} \\t'.format(\n",
    "        len(val_dataloader),\n",
    "        loss=np.mean(val_loss),\n",
    "        mac_acc=mac_acc,\n",
    "        mac_pre=mac_pre,\n",
    "        mac_rec=mac_rec,\n",
    "        mac_f1=mac_f1,\n",
    "        mac_auc=auc_dict['auc_macro'],\n",
    "        mic_acc=mic_acc,\n",
    "        mic_pre=mic_pre,\n",
    "        mic_rec=mic_rec,\n",
    "        mic_f1=mic_f1,\n",
    "        auc_micro=auc_dict['auc_micro'],\n",
    "        prec_at_5=prec_at_5,\n",
    "        prec_at_8=prec_at_8,\n",
    "        prec_at_15=prec_at_15))\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "def accuracy_thresh(y_pred, y_true, thresh:float=0.5, sigmoid:bool=True):\n",
    "    \"Compute accuracy when `y_pred` and `y_true` are the same size.\"\n",
    "    if sigmoid: \n",
    "        y_pred = y_pred.sigmoid()\n",
    "    return ((y_pred>thresh)==y_true.byte()).float().mean().item()\n",
    "    #return np.mean(((y_pred>thresh).float()==y_true.float()).float().cpu().numpy(), axis=1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check for cuda availability and make use of cuda to speed up our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "Device name: GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize and train our BERT model with one epoch. Setting the evaluation to True and train to True so the both training and evaluation will occur. It takes around 1-2 hours to train the model so you probably can go grab a cup of coffee. After training, the model will be saved in the specified filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.3270 \tMacro Accuracy 0.000 \tMacro Recall 0.000 \tMacro Precision 0.000 \tMacro F1 0.000 \tMacro AUC 0.534 \tMicro Accuracy 0.000 \tMicro Recall 0.000 \tMicro Precision nan \tMicro F1 nan \tMicro AUC 0.690 \tP@5 0.282 \tP@8 0.246 \tP@15 0.202 \t\n",
      " 0.327027  |   0.89   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-80-2f464b20be91>:79: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return intersect_size(yhatmic, ymic, 0) / yhatmic.sum(axis=0)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(profile='full')\n",
    "set_seed(42)    # Set seed for reproducibility\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=1)\n",
    "train(bert_classifier, train_dataloader, val_dataloader, epochs=1, evaluation=True, train=True, save=SAVE_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead alot of times you would want to just train the model ones and evaluate it multiple. In this case, you would want to set the train parameter to True and it will load the model from the specified file/save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train(bert_classifier, train_dataloader, val_dataloader, epochs=4, evaluation=True, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following predict function will be used to perform predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_predict(model, test_dataloader):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        all_logits.append(logits)\n",
    "    \n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    #probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "    probs = all_logits.sigmoid().cpu().numpy()\n",
    "    \n",
    "\n",
    "    return probs\n",
    "\n",
    "#probs = all_logits.sigmoid().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the utility function, we can obtain the predicted probability of the specified validation or test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute predicted probabilities on the test set\n",
    "\n",
    "probs = bert_predict(bert_classifier,val_dataloader)\n",
    "\n",
    "# Evalueate the bert classifier\n",
    "\n",
    "# evaluate_roc(probs, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we combine the train and validation set as training data and perform predictions using the testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |  14254  |   0.336295   |     -      |     -     |  2341.04 \n",
      "----------------------------------------------------------------------\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |  14254  |   0.329474   |     -      |     -     |  2342.74 \n",
      "----------------------------------------------------------------------\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-975d4e1a7c91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mbert_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_train_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-77-8eb9ed85f489>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, val_dataloader, epochs, evaluation, train, save)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0;31m# Perform a forward pass. This will return logits.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_attn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0;31m# Compute loss and accumulate the loss values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m         )\n\u001b[0;32m--> 971\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    972\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    566\u001b[0m                 )\n\u001b[1;32m    567\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    569\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    864\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 866\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Concatenate the train set and the validation set\n",
    "\n",
    "full_train_data = torch.utils.data.ConcatDataset([train_data, val_data])\n",
    "full_train_sampler = RandomSampler(full_train_data)\n",
    "full_train_dataloader = DataLoader(full_train_data, sampler=full_train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Train the Bert Classifier on the entire training data\n",
    "set_seed(42)\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=10)\n",
    "train(bert_classifier, full_train_dataloader, epochs=10, evaluation=True, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's all!\n",
    "\n",
    "I have only trained the 5 BERT models once in the interest of time. Since the loss does not improve that much over one epoch, I have decided to just stick with one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
