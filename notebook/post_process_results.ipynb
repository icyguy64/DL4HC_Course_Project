{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation - BERT \n",
    "\n",
    "In this notebook, i'll combine the results of the 5 different BERT models. Initially I did some averaging but the results were not ideal and hence I manually combined them to observe the upper limit of this BERT implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the predictions, labels into dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "y_all_0_df = pd.read_csv('y_all_0.csv',sep=',',header=None)\n",
    "y_all_500_df = pd.read_csv('y_all_500.csv',sep=',',header=None)\n",
    "y_all_1000_df = pd.read_csv('y_all_1000.csv',sep=',',header=None)\n",
    "y_all_1500_df = pd.read_csv('y_all_1500.csv',sep=',',header=None)\n",
    "y_all_2000_df = pd.read_csv('y_all_2000.csv',sep=',',header=None)\n",
    "\n",
    "y_hat_0_df = pd.read_csv('y_hat_0.csv',sep=',',header=None)\n",
    "y_hat_500_df = pd.read_csv('y_hat_500.csv',sep=',',header=None)\n",
    "y_hat_1000_df = pd.read_csv('y_hat_1000.csv',sep=',',header=None)\n",
    "y_hat_1500_df = pd.read_csv('y_hat_1500.csv',sep=',',header=None)\n",
    "y_hat_2000_df = pd.read_csv('y_hat_2000.csv',sep=',',header=None)\n",
    "\n",
    "yhat_raws_0_df = pd.read_csv('yhat_raws_0.csv',sep=',',header=None)\n",
    "yhat_raws_500_df = pd.read_csv('yhat_raws_500.csv',sep=',',header=None)\n",
    "yhat_raws_1000_df = pd.read_csv('yhat_raws_1000.csv',sep=',',header=None)\n",
    "yhat_raws_1500_df = pd.read_csv('yhat_raws_1500.csv',sep=',',header=None)\n",
    "yhat_raws_2000_df = pd.read_csv('yhat_raws_2000.csv',sep=',',header=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 11,404 labels in y_all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11404 entries, 0 to 11403\n",
      "Data columns (total 50 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       11404 non-null  float64\n",
      " 1   1       11404 non-null  float64\n",
      " 2   2       11404 non-null  float64\n",
      " 3   3       11404 non-null  float64\n",
      " 4   4       11404 non-null  float64\n",
      " 5   5       11404 non-null  float64\n",
      " 6   6       11404 non-null  float64\n",
      " 7   7       11404 non-null  float64\n",
      " 8   8       11404 non-null  float64\n",
      " 9   9       11404 non-null  float64\n",
      " 10  10      11404 non-null  float64\n",
      " 11  11      11404 non-null  float64\n",
      " 12  12      11404 non-null  float64\n",
      " 13  13      11404 non-null  float64\n",
      " 14  14      11404 non-null  float64\n",
      " 15  15      11404 non-null  float64\n",
      " 16  16      11404 non-null  float64\n",
      " 17  17      11404 non-null  float64\n",
      " 18  18      11404 non-null  float64\n",
      " 19  19      11404 non-null  float64\n",
      " 20  20      11404 non-null  float64\n",
      " 21  21      11404 non-null  float64\n",
      " 22  22      11404 non-null  float64\n",
      " 23  23      11404 non-null  float64\n",
      " 24  24      11404 non-null  float64\n",
      " 25  25      11404 non-null  float64\n",
      " 26  26      11404 non-null  float64\n",
      " 27  27      11404 non-null  float64\n",
      " 28  28      11404 non-null  float64\n",
      " 29  29      11404 non-null  float64\n",
      " 30  30      11404 non-null  float64\n",
      " 31  31      11404 non-null  float64\n",
      " 32  32      11404 non-null  float64\n",
      " 33  33      11404 non-null  float64\n",
      " 34  34      11404 non-null  float64\n",
      " 35  35      11404 non-null  float64\n",
      " 36  36      11404 non-null  float64\n",
      " 37  37      11404 non-null  float64\n",
      " 38  38      11404 non-null  float64\n",
      " 39  39      11404 non-null  float64\n",
      " 40  40      11404 non-null  float64\n",
      " 41  41      11404 non-null  float64\n",
      " 42  42      11404 non-null  float64\n",
      " 43  43      11404 non-null  float64\n",
      " 44  44      11404 non-null  float64\n",
      " 45  45      11404 non-null  float64\n",
      " 46  46      11404 non-null  float64\n",
      " 47  47      11404 non-null  float64\n",
      " 48  48      11404 non-null  float64\n",
      " 49  49      11404 non-null  float64\n",
      "dtypes: float64(50)\n",
      "memory usage: 4.4 MB\n"
     ]
    }
   ],
   "source": [
    "y_all_0_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There aere 11,404 labels in y_all_500 as well. This should be identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11404 entries, 0 to 11403\n",
      "Data columns (total 50 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       11404 non-null  float64\n",
      " 1   1       11404 non-null  float64\n",
      " 2   2       11404 non-null  float64\n",
      " 3   3       11404 non-null  float64\n",
      " 4   4       11404 non-null  float64\n",
      " 5   5       11404 non-null  float64\n",
      " 6   6       11404 non-null  float64\n",
      " 7   7       11404 non-null  float64\n",
      " 8   8       11404 non-null  float64\n",
      " 9   9       11404 non-null  float64\n",
      " 10  10      11404 non-null  float64\n",
      " 11  11      11404 non-null  float64\n",
      " 12  12      11404 non-null  float64\n",
      " 13  13      11404 non-null  float64\n",
      " 14  14      11404 non-null  float64\n",
      " 15  15      11404 non-null  float64\n",
      " 16  16      11404 non-null  float64\n",
      " 17  17      11404 non-null  float64\n",
      " 18  18      11404 non-null  float64\n",
      " 19  19      11404 non-null  float64\n",
      " 20  20      11404 non-null  float64\n",
      " 21  21      11404 non-null  float64\n",
      " 22  22      11404 non-null  float64\n",
      " 23  23      11404 non-null  float64\n",
      " 24  24      11404 non-null  float64\n",
      " 25  25      11404 non-null  float64\n",
      " 26  26      11404 non-null  float64\n",
      " 27  27      11404 non-null  float64\n",
      " 28  28      11404 non-null  float64\n",
      " 29  29      11404 non-null  float64\n",
      " 30  30      11404 non-null  float64\n",
      " 31  31      11404 non-null  float64\n",
      " 32  32      11404 non-null  float64\n",
      " 33  33      11404 non-null  float64\n",
      " 34  34      11404 non-null  float64\n",
      " 35  35      11404 non-null  float64\n",
      " 36  36      11404 non-null  float64\n",
      " 37  37      11404 non-null  float64\n",
      " 38  38      11404 non-null  float64\n",
      " 39  39      11404 non-null  float64\n",
      " 40  40      11404 non-null  float64\n",
      " 41  41      11404 non-null  float64\n",
      " 42  42      11404 non-null  float64\n",
      " 43  43      11404 non-null  float64\n",
      " 44  44      11404 non-null  float64\n",
      " 45  45      11404 non-null  float64\n",
      " 46  46      11404 non-null  float64\n",
      " 47  47      11404 non-null  float64\n",
      " 48  48      11404 non-null  float64\n",
      " 49  49      11404 non-null  float64\n",
      "dtypes: float64(50)\n",
      "memory usage: 4.4 MB\n"
     ]
    }
   ],
   "source": [
    "y_all_500_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do a simple averaging of the 5 different BERT predictions and observe that the results is not that ideal. Instead, will explore the upper performance limits of this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35444078947368424"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 8\n",
    "# num true labels in top k predictions / k\n",
    "sortd = np.argsort(yhat_raws)[:, ::-1]\n",
    "#print(sortd[0])\n",
    "topk = sortd[:, :k]\n",
    "#print(topk[0])\n",
    "selected = True\n",
    "# get precision at k for each example\n",
    "vals = []\n",
    "\n",
    "for i, y_all_tk in enumerate(y_all):\n",
    "    result = 0.2*yhat_raws_0_df.to_numpy() + 0.2*yhat_raws_500_df.to_numpy() +0.2*yhat_raws_1000_df.to_numpy() +0.2*yhat_raws_1500_df.to_numpy() +0.2*yhat_raws_2000_df.to_numpy()\n",
    "    sortd = np.argsort(result)[:, ::-1]\n",
    "    topk = sortd[:, :k]\n",
    "    #print(topk[0])\n",
    "    num_true_in_top_k = y_all_tk[topk[i]].sum()\n",
    "\n",
    "    vals.append(num_true_in_top_k / k)\n",
    "    \n",
    "np.mean(vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the following code segment will do is to iterate through all the labels of the dataset. Select the BERT output (we have 5 to choose from) and choose the one with least errors. This is to obtain the upper performance limit of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_optim = []\n",
    "for index, row in enumerate(y_all):\n",
    "    #print('new row')\n",
    "    result1 = sum(row != round(yhat_raws_0_df.iloc[index,:]))\n",
    "    result2 = sum(row != round(yhat_raws_500_df.iloc[index,:]))\n",
    "    result3 = sum(row != round(yhat_raws_1000_df.iloc[index,:]))\n",
    "    result4 = sum(row != round(yhat_raws_1500_df.iloc[index,:]))\n",
    "    result5 = sum(row != round(yhat_raws_2000_df.iloc[index,:]))\n",
    "    \n",
    "    result = [result1, result2, result3, result4, result5]\n",
    "    \n",
    "    value = min(result)\n",
    "    #print(result)\n",
    "    #print(value)\n",
    "    #print(result.index(value))\n",
    "    \n",
    "    if result.index(value) == 0:\n",
    "        y_optim.append(yhat_raws_0_df.iloc[index,:])\n",
    "    elif result.index(value) == 1:\n",
    "        y_optim.append(yhat_raws_500_df.iloc[index,:])\n",
    "    elif result.index(value) == 2:\n",
    "        y_optim.append(yhat_raws_1000_df.iloc[index,:])\n",
    "    elif result.index(value) == 3:\n",
    "        y_optim.append(yhat_raws_1500_df.iloc[index,:])\n",
    "    elif result.index(value) == 4:\n",
    "        y_optim.append(yhat_raws_2000_df.iloc[index,:])\n",
    "    #if index == 20:\n",
    "        #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code segment will select and optimize the 5 BERT raw outputs based on precision@k with a specified k value of 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.0, 8.0, 8.0, 8.0, 8.0]\n",
      "[8.0, 8.0, 8.0, 8.0, 8.0]\n",
      "[8.0, 8.0, 8.0, 8.0, 8.0]\n",
      "[8.0, 8.0, 8.0, 8.0, 8.0]\n",
      "[8.0, 8.0, 8.0, 8.0, 8.0]\n",
      "[8.0, 8.0, 8.0, 8.0, 8.0]\n",
      "[8.0, 8.0, 8.0, 8.0, 8.0]\n",
      "[8.0, 8.0, 8.0, 8.0, 8.0]\n",
      "[8.0, 8.0, 8.0, 8.0, 8.0]\n",
      "[8.0, 8.0, 8.0, 8.0, 8.0]\n",
      "[8.0, 8.0, 8.0, 8.0, 8.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4375"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 8\n",
    "# num true labels in top k predictions / k\n",
    "sortd = np.argsort(yhat_raws)[:, ::-1]\n",
    "#print(sortd[0])\n",
    "topk = sortd[:, :k]\n",
    "#print(topk[0])\n",
    "selected = True\n",
    "# get precision at k for each example\n",
    "vals = []\n",
    "for i, y_all_tk in enumerate(y_all):\n",
    "    if selected:\n",
    "        sortd = np.argsort(yhat_raws_0_df.to_numpy())[:, ::-1]\n",
    "        topk1 = sortd[:, :k]\n",
    "        num_true_in_top_k1 = y_all_tk[topk1[i]].sum()\n",
    "\n",
    "        sortd = np.argsort(yhat_raws_500_df.to_numpy())[:, ::-1]\n",
    "        topk2 = sortd[:, :k]\n",
    "        num_true_in_top_k2 = y_all_tk[topk2[i]].sum()\n",
    "\n",
    "        sortd = np.argsort(yhat_raws_1000_df.to_numpy())[:, ::-1]\n",
    "        topk3 = sortd[:, :k]\n",
    "        num_true_in_top_k3 = y_all_tk[topk3[i]].sum()\n",
    "\n",
    "        sortd = np.argsort(yhat_raws_1500_df.to_numpy())[:, ::-1]\n",
    "        topk4 = sortd[:, :k]\n",
    "        num_true_in_top_k4 = y_all_tk[topk4[i]].sum()\n",
    "\n",
    "        sortd = np.argsort(yhat_raws_2000_df.to_numpy())[:, ::-1]\n",
    "        topk5 = sortd[:, :k]\n",
    "        num_true_in_top_k5 = y_all_tk[topk5[i]].sum()\n",
    "\n",
    "        num_true_in_top_k = [num_true_in_top_k1, \n",
    "                             num_true_in_top_k2, \n",
    "                             num_true_in_top_k3, \n",
    "                             num_true_in_top_k4, \n",
    "                             num_true_in_top_k5]\n",
    "        topk_len = [float(len(topk1[i])), \n",
    "                    float(len(topk2[i])), \n",
    "                    float(len(topk3[i])), \n",
    "                    float(len(topk4[i])), \n",
    "                    float(len(topk5[i]))]\n",
    "        print(topk_len)\n",
    "        if i == 10:\n",
    "            break;\n",
    "        #num_true_in_top_k.index(min(num_true_in_top_k))\n",
    "        #print(num_true_in_top_k)\n",
    "        #denom = len(tk)\n",
    "        index = num_true_in_top_k.index(max(num_true_in_top_k))\n",
    "    #print(num_true_in_top_k[index])\n",
    "    #print(topk2[0])\n",
    "    #print(np.unique(np.concatenate((topk1[0],topk2[0]))))\n",
    "        vals.append(num_true_in_top_k[index] / topk_len[index])\n",
    "    else:\n",
    "        sortd = np.argsort(yhat_raws_0_df.to_numpy())[:, ::-1]\n",
    "        topk1 = sortd[:, :k]\n",
    "\n",
    "        sortd = np.argsort(yhat_raws_500_df.to_numpy())[:, ::-1]\n",
    "        topk2 = sortd[:, :k]\n",
    "\n",
    "        sortd = np.argsort(yhat_raws_1000_df.to_numpy())[:, ::-1]\n",
    "        topk3 = sortd[:, :k]\n",
    "\n",
    "        sortd = np.argsort(yhat_raws_1500_df.to_numpy())[:, ::-1]\n",
    "        topk4 = sortd[:, :k]\n",
    "\n",
    "        sortd = np.argsort(yhat_raws_2000_df.to_numpy())[:, ::-1]\n",
    "        topk5 = sortd[:, :k]\n",
    "\n",
    "        combined_topk = np.unique(np.concatenate((topk1[i],topk2[i],topk3[i],topk4[i],topk5[i])))\n",
    "        \n",
    "        num_true_in_top_k = y_all_tk[combined_topk].sum()\n",
    "        vals.append(num_true_in_top_k / float(len(combined_topk)))\n",
    "    \n",
    "np.mean(vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to convert these 3 variables (y_all, yhat_raws, y_hats) into numpy arrays for our subsequent codes to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all = y_all_0_df.to_numpy() \n",
    "yhat_raws = yhat_raws_0_df.to_numpy() #+ 0.3*yhat_raws_500_df.to_numpy() \n",
    "y_hats = yhat_raws.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hats = round(pd.DataFrame(y_optim)).to_numpy()\n",
    "yhat_raws = pd.DataFrame(y_optim).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the utility functions used to calculate Macro/Micro AUC, Macro/Micro F1, Precision@k. We'll make use of them again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Helper class that I found online, it's pretty good. Just computes the average. \n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "#  referenced from James Mullenbach, https://github.com/jamesmullenbach/caml-mimic/\n",
    "def union_size(yhat, y, axis):\n",
    "    # axis=0 for label-level union (macro). axis=1 for instance-level\n",
    "    return np.logical_or(yhat, y).sum(axis=axis).astype(float)\n",
    "\n",
    "#  referenced from James Mullenbach, https://github.com/jamesmullenbach/caml-mimic/\n",
    "def intersect_size(yhat, y, axis):\n",
    "    # axis=0 for label-level union (macro). axis=1 for instance-level\n",
    "    return np.logical_and(yhat, y).sum(axis=axis).astype(float)\n",
    "\n",
    "#  referenced from James Mullenbach, https://github.com/jamesmullenbach/caml-mimic/evaluation.py\n",
    "def macro_accuracy(yhat, y):\n",
    "    num = intersect_size(yhat, y, 0) / (union_size(yhat, y, 0) + 1e-10)\n",
    "    return np.mean(num)\n",
    "\n",
    "#  referenced from James Mullenbach, https://github.com/jamesmullenbach/caml-mimic/evaluation.py\n",
    "def macro_precision(yhat, y):\n",
    "    num = intersect_size(yhat, y, 0) / (yhat.sum(axis=0) + 1e-10)\n",
    "    return np.mean(num)\n",
    "\n",
    "#  referenced from James Mullenbach, https://github.com/jamesmullenbach/caml-mimic/evaluation.py\n",
    "def macro_recall(yhat, y):\n",
    "    num = intersect_size(yhat, y, 0) / (y.sum(axis=0) + 1e-10)\n",
    "    return np.mean(num)\n",
    "\n",
    "#  referenced from James Mullenbach, https://github.com/jamesmullenbach/caml-mimic/evaluation.py\n",
    "def macro_f1(yhat, y):\n",
    "    prec = macro_precision(yhat, y)\n",
    "    rec = macro_recall(yhat, y)\n",
    "    if prec + rec == 0:\n",
    "        f1 = 0.\n",
    "    else:\n",
    "        f1 = 2 * (prec * rec) / (prec + rec)\n",
    "    return f1\n",
    "\n",
    "#  referenced from James Mullenbach, https://github.com/jamesmullenbach/caml-mimic/evaluation.py\n",
    "def micro_accuracy(yhatmic, ymic):\n",
    "    return intersect_size(yhatmic, ymic, 0) / union_size(yhatmic, ymic, 0)\n",
    "\n",
    "#  referenced from James Mullenbach, https://github.com/jamesmullenbach/caml-mimic/evaluation.py\n",
    "def micro_precision(yhatmic, ymic):\n",
    "    return intersect_size(yhatmic, ymic, 0) / yhatmic.sum(axis=0)\n",
    "\n",
    "#  referenced from James Mullenbach, https://github.com/jamesmullenbach/caml-mimic/evaluation.py\n",
    "def micro_recall(yhatmic, ymic):\n",
    "    return intersect_size(yhatmic, ymic, 0) / ymic.sum(axis=0)\n",
    "\n",
    "#  referenced from James Mullenbach, https://github.com/jamesmullenbach/caml-mimic/evaluation.py\n",
    "def micro_f1(yhatmic, ymic):\n",
    "    prec = micro_precision(yhatmic, ymic)\n",
    "    rec = micro_recall(yhatmic, ymic)\n",
    "    if prec + rec == 0:\n",
    "        f1 = 0.\n",
    "    else:\n",
    "        f1 = 2 * (prec * rec) / (prec + rec)\n",
    "    return f1\n",
    "\n",
    "#  referenced from James Mullenbach, https://github.com/jamesmullenbach/caml-mimic/evaluation.py\n",
    "def inst_precision(yhat, y):\n",
    "    num = intersect_size(yhat, y, 1) / yhat.sum(axis=1)\n",
    "    # correct for divide-by-zeros\n",
    "    num[np.isnan(num)] = 0.\n",
    "    return np.mean(num)\n",
    "\n",
    "#  referenced from James Mullenbach, https://github.com/jamesmullenbach/caml-mimic/evaluation.py\n",
    "def inst_recall(yhat, y):\n",
    "    num = intersect_size(yhat, y, 1) / y.sum(axis=1)\n",
    "    # correct for divide-by-zeros\n",
    "    num[np.isnan(num)] = 0.\n",
    "    return np.mean(num)\n",
    "\n",
    "#  referenced from James Mullenbach, https://github.com/jamesmullenbach/caml-mimic/evaluation.py\n",
    "def inst_f1(yhat, y):\n",
    "    prec = inst_precision(yhat, y)\n",
    "    rec = inst_recall(yhat, y)\n",
    "    f1 = 2 * (prec * rec) / (prec + rec)\n",
    "    return f1\n",
    "\n",
    "#  referenced from James Mullenbach, https://github.com/jamesmullenbach/caml-mimic/evaluation.py\n",
    "def precision_at_k(yhat_raw, y, k):\n",
    "    # num true labels in top k predictions / k\n",
    "    sortd = np.argsort(yhat_raw)[:, ::-1]\n",
    "    topk = sortd[:, :k]\n",
    "\n",
    "    # get precision at k for each example\n",
    "    vals = []\n",
    "    for i, tk in enumerate(topk):\n",
    "        if len(tk) > 0:\n",
    "            num_true_in_top_k = y[i, tk].sum()\n",
    "            denom = len(tk)\n",
    "            vals.append(num_true_in_top_k / float(denom))\n",
    "\n",
    "    return np.mean(vals)\n",
    "\n",
    "\n",
    "#  referenced from James Mullenbach, https://github.com/jamesmullenbach/caml-mimic/evaluation.py\n",
    "def auc_metrics(yhat_raw, y, ymic):\n",
    "    if yhat_raw.shape[0] <= 1:\n",
    "        return\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = {}\n",
    "    # get AUC for each label individually\n",
    "    relevant_labels = []\n",
    "    auc_labels = {}\n",
    "    for i in range(y.shape[1]):\n",
    "        # only if there are true positives for this label\n",
    "        if y[:, i].sum() > 0:\n",
    "            fpr[i], tpr[i], _ = roc_curve(y[:, i], yhat_raw[:, i])\n",
    "            if len(fpr[i]) > 1 and len(tpr[i]) > 1:\n",
    "                auc_score = auc(fpr[i], tpr[i])\n",
    "                if not np.isnan(auc_score):\n",
    "                    auc_labels[\"auc_%d\" % i] = auc_score\n",
    "                    relevant_labels.append(i)\n",
    "\n",
    "    # macro-AUC: just average the auc scores\n",
    "    aucs = []\n",
    "    for i in relevant_labels:\n",
    "        aucs.append(auc_labels['auc_%d' % i])\n",
    "    roc_auc['auc_macro'] = np.mean(aucs)\n",
    "\n",
    "    # micro-AUC: just look at each individual prediction\n",
    "    yhatmic = yhat_raw.ravel()\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(ymic, yhatmic)\n",
    "    roc_auc[\"auc_micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code segment will calculate the performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Accuracy 0.186 \tMacro Recall 0.203 \tMacro Precision 0.478 \tMacro F1 0.285 \tMacro AUC 0.801 \tMicro Accuracy 0.259 \tMicro Recall 0.273 \tMicro Precision 0.834 \tMicro F1 0.411 \tMicro AUC 0.846 \tP@5 0.480 \tP@8 0.383 \tP@15 0.269 \t\n"
     ]
    }
   ],
   "source": [
    "#print('y_hats')\n",
    "#print(y_hats.shape)\n",
    "num_rows = int(y_hats.shape[0])\n",
    "#num_cols = int(y_hats.shape[1])\n",
    "#print(num_rows)\n",
    "#print(num_cols)\n",
    "num_rows_batch_1 = int(y_hats.shape[0]/5)\n",
    "num_rows_batch_2 = int(y_hats.shape[0]/5*2)\n",
    "num_rows_batch_3 = int(y_hats.shape[0]/5*3)\n",
    "num_rows_batch_4 = int(y_hats.shape[0]/5*4)\n",
    "num_rows_batch_5 = int(y_hats.shape[0]/5*5)\n",
    "\n",
    "#y_hats = y_hats[0:num_rows_batch_1,:] #+ y_hats[num_rows_batch_1:num_rows_batch_2,:] + y_hats[num_rows_batch_2:num_rows_batch_3,:] + y_hats[num_rows_batch_3:num_rows_batch_4,:] + y_hats[num_rows_batch_4:num_rows,:]\n",
    "#y_all = y_all[0:num_rows_batch_1,:] #+ y_all[num_rows_batch_1:num_rows_batch_2,:] + y_all[num_rows_batch_2:num_rows_batch_3,:] + y_all[num_rows_batch_3:num_rows_batch_4,:] + y_all[num_rows_batch_4:num_rows,:]\n",
    "#yhat_raws = yhat_raws[0:num_rows_batch_1,:] #+ yhat_raws[num_rows_batch_1:num_rows_batch_2,:] + yhat_raws[num_rows_batch_2:num_rows_batch_3,:] + yhat_raws[num_rows_batch_3:num_rows_batch_4,:] + yhat_raws[num_rows_batch_4:num_rows,:]\n",
    "\n",
    "#y_hats[y_hats > 1] = 1\n",
    "#y_all[y_all > 1] = 1\n",
    "#yhat_raws[yhat_raws > 1] = 1\n",
    "\n",
    "mac_acc = macro_accuracy(y_hats, y_all)\n",
    "mac_rec = macro_recall(y_hats, y_all)\n",
    "mac_pre = macro_precision(y_hats, y_all)\n",
    "mic_acc = micro_accuracy(y_hats.ravel(), y_all.ravel())\n",
    "mic_rec = micro_recall(y_hats.ravel(), y_all.ravel())\n",
    "mic_pre = micro_precision(y_hats.ravel(), y_all.ravel())\n",
    "mic_f1 = micro_f1(y_hats.ravel(), y_all.ravel())\n",
    "mac_f1 = macro_f1(y_hats, y_all)\n",
    "auc_dict = auc_metrics(yhat_raws, y_all, y_all.ravel())\n",
    "prec_at_5 = precision_at_k(yhat_raws, y_all, 5)\n",
    "prec_at_8 = precision_at_k(yhat_raws, y_all, 8)\n",
    "prec_at_15 = precision_at_k(yhat_raws, y_all, 15)\n",
    "\n",
    "print('Macro Accuracy {mac_acc:.3f} \\t'\n",
    "      'Macro Recall {mac_rec:.3f} \\t'\n",
    "      'Macro Precision {mac_pre:.3f} \\t'\n",
    "      'Macro F1 {mac_f1:.3f} \\t'\n",
    "      'Macro AUC {mac_auc:.3f} \\t'\n",
    "      'Micro Accuracy {mic_acc:.3f} \\t'\n",
    "      'Micro Recall {mic_rec:.3f} \\t'\n",
    "      'Micro Precision {mic_pre:.3f} \\t'\n",
    "      'Micro F1 {mic_f1:.3f} \\t'\n",
    "      'Micro AUC {auc_micro:.3f} \\t'\n",
    "      'P@5 {prec_at_5:.3f} \\t'\n",
    "      'P@8 {prec_at_8:.3f} \\t'\n",
    "      'P@15 {prec_at_15:.3f} \\t'.format(\n",
    "    mac_acc=mac_acc,\n",
    "    mac_pre=mac_pre,\n",
    "    mac_rec=mac_rec,\n",
    "    mac_f1=mac_f1,\n",
    "    mac_auc=auc_dict['auc_macro'],\n",
    "    mic_acc=mic_acc,\n",
    "    mic_pre=mic_pre,\n",
    "    mic_rec=mic_rec,\n",
    "    mic_f1=mic_f1,\n",
    "    auc_micro=auc_dict['auc_micro'],\n",
    "    prec_at_5=prec_at_5,\n",
    "    prec_at_8=prec_at_8,\n",
    "    prec_at_15=prec_at_15))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing remarks\n",
    "\n",
    "Now that we observe that the upper limit of this current 5 BERT, 500 words each approach achieves a Macro AUC score of 0.801, Micro AUC score of 0.846, Macro F1 of 0.285, Micro F1 of 0.411, P@8 of 0.383, P@15 of 0.269. Probably will have to explore other implementations in order to achieve a better score compared to LR/CNN/BiGRU/CAML. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
